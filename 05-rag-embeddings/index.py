#安装open api
#pip install --upgrade openai
# 安装 pdf 解析库
#pip install pdfminer.six
#安装elastic搜索引擎 安装 ES 客户端
#pip3 install elasticsearch8
#安装 NLTK（文本处理方法库）
#pip install nltk
import os
import re
import nltk
import warnings
import json
import requests
import os
import chromadb
from chromadb.config import Settings
from elasticsearch8 import Elasticsearch, helpers
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from pdfminer.high_level import extract_pages
from pdfminer.layout import LTTextContainer
from typing import Dict, Any

# response: Dict[str, Any] = requests.get('https://api.github.com').json()

os.environ['ES_DEBUG'] = 'True'

warnings.simplefilter("ignore")  # 屏蔽 ES 的一些Warnings

# nltk.download('punkt')  # 英文切词、词根、切句等方法
# nltk.download('stopwords')  # 英文停用词库
#从 PDF 文件中（按指定页码）提取文字
def extract_text_from_pdf(filename, page_numbers=None, min_line_length=1):
    '''从 PDF 文件中（按指定页码）提取文字'''
    paragraphs = []
    buffer = ''
    full_text = ''
    # 提取全部文本
    for i, page_layout in enumerate(extract_pages(filename)):
        # 如果指定了页码范围，跳过范围外的页
        if page_numbers is not None and i not in page_numbers:
            continue
        for element in page_layout:
            if isinstance(element, LTTextContainer):
                full_text += element.get_text() + '\n'
    # 按空行分隔，将文本重新组织成段落
    lines = full_text.split('\n')
    for text in lines:
        if len(text) >= min_line_length:
            buffer += (' '+text) if not text.endswith('-') else text.strip('-')
        elif buffer:
            paragraphs.append(buffer)
            buffer = ''
    if buffer:
        paragraphs.append(buffer)
    return paragraphs
def printExmp1(filename,min_line_length):
    #从llama2.pdf文件中提取文字  ".\\05-rag-embeddings\\PDF\\llama2.pdf" 
    paragraphs = extract_text_from_pdf(filename, None,min_line_length)
    #查看打印文字
    for para in paragraphs[:3]:
        print(para+"\n")
    return paragraphs  
def to_keywords(input_string):
    '''（英文）文本只保留关键字'''
    # 使用正则表达式替换所有非字母数字的字符为空格
    no_symbols = re.sub(r'[^a-zA-Z0-9\s]', ' ', input_string)
    word_tokens = word_tokenize(no_symbols)
    # 加载停用词表
    stop_words = set(stopwords.words('english'))
    ps = PorterStemmer()
    # 去停用词，取词根
    filtered_sentence = [ps.stem(w)
                         for w in word_tokens if not w.lower() in stop_words]
    return ' '.join(filtered_sentence)

def elasticOpen(): 
    # Password for the 'elastic' user generated by Elasticsearch
    ELASTIC_PASSWORD = "FbG0wXlppDhngtg92PGT5nwp"
    # Found in the 'Manage Deployment' page
    CLOUD_ID = "a8d79c8ad570482188d3d12a57532ea7:dXMtY2VudHJhbDEuZ2NwLmNsb3VkLmVzLmlvOjQ0MyRlOGJjY2FlZDUwNGY0NmNjOGM5ZWFhNmZjMGMyYjJhYiRkZDQ5MTRkODdjZDY0YTJjYTk3NTYzMjc2ODFiMGE1Yg=="

    # Create the client instance
    es = Elasticsearch(
        cloud_id=CLOUD_ID,
        basic_auth=("elastic", ELASTIC_PASSWORD)
    )
    # 1. 创建Elasticsearch连接
    # Successful response!
    es.info()
    return es
 
#参数paragraphs值从调用函数获取extract_text_from_pdf(filename, page_numbers=None, min_line_length=1)
#llama2.pdf文件中提取文字  ".\\05-rag-embeddings\\PDF\\llama2.pdf" 
index_name = "teacher_demo_index_tmp0621"
def elasticBulk(paragraphs):
    es = elasticOpen()
    # 2. 定义索引名称
    index_name = "teacher_demo_index_tmp0621"
    # 3. 如果索引已存在，删除它（仅供演示，实际应用时不需要这步）
    if es.indices.exists(index=index_name):
        print(f"Index '{index_name}' already exists.")
        es.indices.delete(index=index_name)
    else:
    # 4. Create index
        try:
            # 4. 创建索引
            es.indices.create(index=index_name)
            print(f"Index '{index_name}' created successfully.")
        except Exception as e:
            print(f"Error creating index '{index_name}': {e}")
            raise
    # 5. 灌库指令
    actions = [
        {
            "_index": index_name,
            "_source": {
                "keywords": to_keywords(para),
                "text": para
            }
        }
        for para in paragraphs
    ]

    # 6. 文本灌库
    helpers.bulk(es, actions)
 

    
#实现关键字检索
def search(query_string, top_n=3):
    es = elasticOpen()
    # ES 的查询语言
    search_query = {
        "match": {
            "keywords": to_keywords(query_string)
        }
    }
    res = es.search(index=index_name, query=search_query, size=top_n)
    return [hit["_source"]["text"] for hit in res["hits"]["hits"]]

# LLM 接口封装
# pip install python-dotenv openai
#调用国产大模型 文心千帆
# 通过鉴权接口获取 access token
def get_access_token():
    """
    使用 AK，SK 生成鉴权签名（Access Token）
    :return: access_token，或是None(如果错误)
    """
    url = "https://aip.baidubce.com/oauth/2.0/token"
    params = {
        "grant_type": "client_credentials",
        "client_id": os.getenv('ERNIE_CLIENT_ID'),
        "client_secret": os.getenv('ERNIE_CLIENT_SECRET')
    }

    return str(requests.post(url, params=params).json().get("access_token"))
# 调用文心千帆 调用 BGE Embedding 接口
def get_embeddings_bge(prompts):
    # url = "https://aip.baidubce.com/rpc/2.0/ai_custom/v1/wenxinworkshop/embeddings/bge_large_en?access_token=" + get_access_token()
    print("000000000"+str(get_access_token()))
    url = "https://aip.baidubce.com/rpc/2.0/ai_custom/v1/wenxinworkshop/embeddings/bge_large_en?access_token=0x0000020F1F411760"
    print("url=>"+url)
    payload = json.dumps({
        "input": prompts
    })
    headers = {'Content-Type': 'application/json'}

    response = requests.request(
        "POST", url, headers=headers, data=payload).json()
    data = response["data"]
    return [x["embedding"] for x in data]
# 调用文心4.0对话接口
def get_completion_ernie(prompt):
    url = "https://aip.baidubce.com/rpc/2.0/ai_custom/v1/wenxinworkshop/chat/completions_pro?access_token=" + get_access_token()
    payload = json.dumps({
        "messages": [
            {
                "role": "user",
                "content": prompt
            }
        ]
    })
    headers = {'Content-Type': 'application/json'}

    response = requests.request(
        "POST", url, headers=headers, data=payload).json()

    return response["result"]
#连接向量数据库
class MyVectorDBConnector:
    def __init__(self, collection_name, embedding_fn):
        chroma_client = chromadb.Client(Settings(allow_reset=True))
        # 为了演示，实际不需要每次 reset()
        chroma_client.reset()

        # 创建一个 collection
        self.collection = chroma_client.get_or_create_collection(
            name=collection_name)
        self.embedding_fn = embedding_fn
        print("sssssssssss--->"+str(embedding_fn))
    def get_inter_access_token(self):
        """
        使用 AK，SK 生成鉴权签名（Access Token）
        :return: access_token，或是None(如果错误)
        """
        url = "https://aip.baidubce.com/oauth/2.0/token"
        params = {
            "grant_type": "client_credentials",
            "client_id": os.getenv('ERNIE_CLIENT_ID'),
            "client_secret": os.getenv('ERNIE_CLIENT_SECRET')
        }
        print("uuuuuuuuuuuu==>"+str(requests.post(url, params=params).json().get("access_token")))
        return str(requests.post(url, params=params).json().get("access_token"))
    def add_documents(self, documents):
        '''向 collection 中添加文档与向量'''
        self.collection.add(
            embeddings=self.embedding_fn(documents),  # 每个文档的向量
            documents=list(documents),  # 文档的原文
            ids=[f"id{i}" for i in range(len(documents))]  # 每个文档的 id
        )
        print("向 collection 中添加文档="+documents)
    def search(self, query, top_n):
        '''检索向量数据库'''
        results = self.collection.query(
            query_embeddings=self.embedding_fn([query]),
            n_results=top_n
        )
        return results
class RAG_Bot:
    def __init__(self, vector_db, llm_api, n_results=2):
        self.vector_db = vector_db
        self.llm_api = llm_api
        self.n_results = n_results
    def chat(self, user_query):
        # 1. 检索
        search_results = self.vector_db.search(user_query, self.n_results)
        # 2. 构建 Prompt
        prompt = build_prompt(
            prompt_template, context=search_results['documents'][0], query=user_query)

        # 3. 调用 LLM
        response = self.llm_api(prompt)
        return response
#PROMPT 模版
def build_prompt(prompt_template, **kwargs):
    '''将 Prompt 模板赋值'''
    inputs = {}
    for k, v in kwargs.items():
        if isinstance(v, list) and all(isinstance(elem, str) for elem in v):
            val = '\n\n'.join(v)
        else:
            val = v
        inputs[k] = val

prompt_template = """
你是一个问答机器人。
你的任务是根据下述给定的已知信息回答用户问题。

已知信息:
{context}

用户问：
{query}

如果已知信息不包含用户问题的答案，或者已知信息不足以回答用户的问题，请直接回复"我无法回答您的问题"。
请不要输出已知信息中不包含的信息或答案。
请用中文回答用户问题。
"""
if __name__ == "__main__":
#     搭建过程：
# 1. 文档加载，并按一定条件**切割**成片段
# 2. 将切割的文本片段灌入**检索引擎**
# 3. 封装**检索接口**
# 4. 构建**调用流程**：Query -> 检索 -> Prompt -> LLM -> 回复
    # Your code here
    # elasticOpen()
    # str1="how many parameters does llama 2 have?"
    # 1. 文档加载，并按一定条件**切割**成片段
    # para1=printExmp1(".\\05-rag-embeddings\\PDF\\llama2.pdf",10)
    #  2. 将切割的文本片段灌入**检索引擎**
    # elasticBulk(para1)
    # 3. 封装**检索接口**
    # ret1=search(str1,5)
    # print("ES 搜索结果="+str(ret1))
    #通过鉴权接口获取 access token
    user_query = "how many parameters does llama 2 have?"
    # 创建一个向量数据库对象
    new_vector_db = MyVectorDBConnector(
        "demo_ernie0621",
        embedding_fn=MyVectorDBConnector.get_inter_access_token
    )
    # 向向量数据库中添加文档
    new_vector_db.add_documents(list(extract_text_from_pdf(".\\05-rag-embeddings\\PDF\\06地球毁灭记-五次生物大灭绝7.pdf", None,10)))

    # 创建一个RAG机器人
    # new_bot = RAG_Bot(
    #     new_vector_db,
    #     llm_api=get_completion_ernie
    # )
    
    # # 1. 检索
    # search_results = search(user_query, 2)
    # # 2. 构建 Prompt
    # prompt = build_prompt(prompt_template, context=search_results, query=user_query)
    # print("===Prompt===")
    # print(prompt)
    # # 3. 调用 LLM
    # response = get_completion(prompt)
    # print("===回复===")
    # print(response)
    # str2=get_access_token()
    # print("access_token=>"+str(str3))